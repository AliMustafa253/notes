TENSOR FLOW

	-Artificial Intelligence: A field of computer science that aims to make computers achieve
			human-style intelligence. 
			There are many approaches to reaching this goal, 
			including machine learning and deep learning.
			
===:-== ----Machine Learning---- ==-:===
	Input and Output Known, rest algorithm needs to be learned.
	software has algorithm known
		
	- Feature — The input(s) to our model.
	- Labels — The output our model predicts.
	- Example — A pair of inputs/outputs used during training.
	
	- Loss function — 
		A way of measuring how far off predictions are from the desired outcome. 
		(The measured difference is called the "loss".)		mean squared error etc
		lower loss = better (closer out come values)
		sparse categorical crossentropy = classification
	
	- Optimizer function — 
		A way of adjusting internal values in order to reduce the loss.
		This optimization process is called Gradient Descent. adam optimizer
		
	- Learning Rate -
		 This is the step size taken when adjusting values in the model. If the value is too small,
		 it will take too many iterations to train the model. Too large, and accuracy goes down.
			
	- Dense Layer = Fully connected to previous layer
	
	- Weight and Biases
		can be changed
	- Normalize between 0-1
	
	28x28=784 pixel
	- Flattening
		2d image into vector
	
	- Activation Function -
		-ReLU gives the network the ability to solve nonlinear problems. 0 for negative max(0,x)
	 	-Softmax uses sparese categorical crossentropy as loss ftn
		-sigmoid = binary crossentropy
		A function that provides probabilities for each possible output class
	- Optimizer e.g Adam
		
	
========================

	----Test Data and Training Data----
	
	Validation dataset
		after every training epoch checks on validation set.
			Validation set = not used for training only tells how well training is doing.
			it can tell if we are overfitting or not
			lowest validation loss = best set
	
	Regression 
		find 1 number, best result given question   For example, an estimate of a house’s value.
	
	Classification = 
		Confidence an item is of one class, output consisting of data.
		
		
========================CNN====================

	Period Dense Networks earlier
	----CNN----
			
	-Invariance
		Recognition of image in diff positions to recognize stuff
	-Convolution:	
		The process of applying a kernel (filter) to an image
		
		-Kernel/filter
		6x6
			Kernel convolution 3x3 multiply with value selected 
			sum all of them to make corresponding value for all pixels
		
		-Padding: Adding pixels of some value, usually 0, around the input image
		
	-Max Pooling-
		A pooling process in which many values are converted into
		a single value by taking the maximum value from among them.
			Max pooling 2x2 stride 2
		needed for Downsampling = reduce input complexity
			
			
	More epochs = overfitting
	
	Overfitting
	
	-images of different sizes
		need fixed size input in NN
		resizing images to 150x150 in dog and cats
		
	--color images--
		3d array
			RGB image = 3 arrays of 2d
			each has its own 2d array
		
		-Convolution on 3d image:
			3d filter depth = matches color channel
			bias added = usually 1
		
		-Max Pooling in 3d image
			On each convoluted output
			
	-image augmentation = zoom, rotate etc if low image count
	
	-Dropout:
		randomly turn off some neurons during training
		so that some neurons are not focused on.
		resistant
		
		