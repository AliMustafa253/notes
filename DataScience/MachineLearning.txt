TENSOR FLOW

	-Artificial Intelligence: A field of computer science that aims to make computers achieve
			human-style intelligence. 
			There are many approaches to reaching this goal, 
			including machine learning and deep learning.
			
===:-== ----Machine Learning---- ==-:===
	Input and Output Known, rest algorithm needs to be learned.
	software has algorithm known
		
	- Feature — The input(s) to our model.
	- Labels — The output our model predicts.
	- Example — A pair of inputs/outputs used during training.
	
	- Loss function — 
		A way of measuring how far off predictions are from the desired outcome. 
		(The measured difference is called the "loss".)		mean squared error etc
		lower loss = better (closer out come values)
		sparse categorical crossentropy = classification
	
	- Optimizer function — 
		A way of adjusting internal values in order to reduce the loss.
		This optimization process is called Gradient Descent. adam optimizer
		
	- Learning Rate -
		 This is the step size taken when adjusting values in the model. If the value is too small,
		 it will take too many iterations to train the model. Too large, and accuracy goes down.
			
	- Dense Layer = Fully connected to previous layer
	
	- Weight and Biases
		can be changed
	- Normalize between 0-1
	
	28x28=784 pixel
	- Flattening
		2d image into vector
	
	- Activation Function -
		-ReLU gives the network the ability to solve nonlinear problems. 0 for negative max(0,x)
	 	-Softmax uses sparese categorical crossentropy as loss ftn
		-sigmoid = binary crossentropy
		A function that provides probabilities for each possible output class
	- Optimizer e.g Adam
		
	
========================

	----Test Data and Training Data----
	
	Validation dataset
		after every training epoch checks on validation set.
			Validation set = not used for training only tells how well training is doing.
			it can tell if we are overfitting or not
			lowest validation loss = best set
	
	Regression 
		find 1 number, best result given question   For example, an estimate of a house’s value.
	
	Classification = 
		Confidence an item is of one class, output consisting of data.
		
		
========================CNN====================

	Period Dense Networks earlier
	----CNN----
			
	-Invariance
		Recognition of image in diff positions to recognize stuff
	-Convolution:	
		The process of applying a kernel (filter) to an image
		
		-Kernel/filter
		6x6
			Kernel convolution 3x3 multiply with value selected 
			sum all of them to make corresponding value for all pixels
		
		-Padding: Adding pixels of some value, usually 0, around the input image
		
	-Max Pooling-
		A pooling process in which many values are converted into
		a single value by taking the maximum value from among them.
			Max pooling 2x2 stride 2
		needed for Downsampling = reduce input complexity
			
			
	More epochs = overfitting
	
	Overfitting
	
	-images of different sizes
		need fixed size input in NN
		resizing images to 150x150 in dog and cats
		
	--color images--
		3d array
			RGB image = 3 arrays of 2d
			each has its own 2d array
		
		-Convolution on 3d image:
			3d filter depth = matches color channel
			bias added = usually 1
		
		-Max Pooling in 3d image
			On each convoluted output
			
	-image augmentation = zoom, rotate etc if low image count
	
	-Dropout:
		randomly turn off some neurons during training
		so that some neurons are not focused on.
		resistant
	
	============Transfer Learning==============
	
	- changed last layer of pretrained model
	- each dataset has different outputs
	
	- frozen layers
	- features learned will changed from new classifier (untrained)
	
	mobilenet - convulated model
	tensorflow hub 
	
	
	=============Transformer================
	Sequence to sequence  architectures:
		defined ordering.
		Help solve nlp problems
	--word embeddings: 
		convert words into numbers 
		inputs connected to active functions and weights
			depend on backpropogation
			initially random number
	--Positional encoding:
		1 example sine cosine used for embedding values
		
	--Self Attention:
		How similar a word is to all other words
		
		-query value
		made from sets of new values.
		query and key e.g dot product to check similarities
		
		similar score put into softmax core
	-residual connection

	--Decoder Encoder Attention:
		Decoder should know significant words in input
		
		
	===========================================
	----Transformers-----
			Can pay attention.
			
			GPT generative pre trained transformer model - 3
			
		Encoder and a Decoder
		Sequence to sequence learning. predicts next word. encoder -> generates encoding to see which part is relevant.
		Decoder takes encodings and makes context.
		
		Semi Supervised learning.
			Fine tuned through supervised learning earlier large data with unsupervised.
		
		--Attention mechanism--
			tries to understand context. RNN must run in sequence.
			Can run in parallel so much better learning times.
			
		
	--BERT--
		
		Encoders 
	
	--GPT--
		Stack decoders for GPT
		
		-Pre training: 
			Train gpt architecture
			Language Modelling---> predict next language could base
			self supervised task
		
		-Fine Tuning: XXXXX
			Provide training data for
				Too much data needed.
				Overfitting easy
		-Zero Shot learning
			What instruction should be done with the question
		-One shot Learning
			Full examples
		-Few Shot learning
			
			
			
=====LLM======

	Essentially, learn most of the internet things and let machine learn things.
	understands questions and gives an appropriate response.
	deep learning. --> Transformers
	
	Larger than other models.
	General Purpose.
	Ability to be pre trained and fine tuned.

	Parameters --> tiny bits of information to understand conversations. Massive amount of parameters in LLM
	
	LLM --> general purpose --> wide variety of text data from the internet.
	Can fine tune later
	
	basics on language first --> customized by fine tuning
	Few shot scenario --> minial data to train model.
	zero-shot --> no additional data
	
	Transformers = subset of deep learning.
	
	--Deep Learning--
		Excel at finding patterns in complex data due to network structure
		Each neuron has math function with weights. updated using optimization.
		
		CNN
			Vision classification --> struggles at language.
			
		RNN
			Good for nlp before neural networks
			not good for long inputs. miss context
			cannot be run parallel due to missing context. too long
			
		---> Transformers ---> can train in a better way. superior understanding.






	
		
	
		
		