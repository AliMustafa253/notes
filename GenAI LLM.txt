Generative AI use cases, project lifecycle, and model pre-training

Fine-tuning and evaluating large language models

Reinforcement learning and LLM-powered applications


===============================================================
How an LLM works:

	- Transformer architecture why it took off etc.
	
	- Gen AI project life cycle.
		foundation model off the shelf, or make my own model etc.
		


	
----------------------------------------------------------
	Gen AI --> subset of ML
		---> learn patterns in datasets of contents generated by humans.
		---> foundation models, BERT,GPT,LLaMa,FLANT-5, PALM
		
		LLM take human written instructions --> prompt
		Context window ---> like a few 1000 windows.
		
		Completion ---> complete prompts
		
	---------
	
	Chat tasks ---> Next word prediction: Basic Chatbot
	
	translation/natural lang to code/file to summary etc/information retrieval (NER)/
	augmenting llms with integrate with apis.
	
	-----------
	
	RNN --> limited. needs more scaling and needs to see more
			than prev few words
			
	
	ATTENTION IS ALL YOU NEED
	
	
	--------------
	Transformers
	--------------
	learns context of all words. applies weights.
	
	- attention weights applied to each other word in sentences during llm training.
	
	- improved encoding language ability a lot
	
	- ****Self Attention
			strongly connected to or paying attention to something.
	
	- Transformers Architecture 
		----> Encoder and Decoder.
		
		big statistical calculators. must tokenize words into numbers.
		
	****tokenization methods --> token Ids, to represent part of word or whole words.
	
	Embedding space ----> Each token is represented as a vector:
	Vector ---> learn to encode meaning and context of each individual sequence.
	
	eg vector of just 3 --> relate words closer calc diff with angles etc.
	
	positional encoding: ---> preserve word order and position of word in sentence.
	
	****Token embeddings + positional encoding.
	-----------
	***GOES TO SELF ATTENTION LAYER ---> Encoder and Decoder
	
	Multi-headed Self Attention. ---> multiple heads learn indibivdually.
					each self attention head learns diff aspects 12-100 may be possible.
	-----------			
	*****GOES TO FEED FORWARD NETWORK 
		Vector of logics for probability of each token.
		final softmax layer ---> probability for each word in vocabulary.
		one token --> higher prob.
		
		feed-forward network applies a point-wise fully connected layer
		to each position separately and identically. 
	
	================================
	Translation using Transformers:
	Sequence to sequence task
		encoder deep representation of structure and meaning of sentence
		--> decoder	startup input trigger to predict next token.
		Output --> through feedforward layer to softmax output.
		New token and new token put forward.
		
		
	****Encoder:
		Encodes inputs (prompts) with contextual understanding
			produces one vector per input token.
			
	****Decoder:
		accepts input tokens and generates new tokens.
		
	BERT = ENCODER ONLY MODEL.
	
	BART and T5 === Encoder decoder models.
	
	GPT, LLaMa === DECODER ONLY MODELS.
		
		
	------------------------------------
	Context Window = total size allowed for prompt.
	
	**in-context learning
	
		zero shot inference.
		
		one shot inference.
		
		few-shot inference.
		
	***configuration parameters.
		Different than training parameters --> give u control over things etc.
		
		max new tokens === cap on how many times it can go back on the model.
	
		softmax of words and probability. has complete dictionary.
		
		greedy decoding --> pick highest probability --> good for short generation.
		more natural ---> random sampling --> (weighted with probability).
	do_sample = true
	***Sample top K and Top P
		Top K ---> only choose from top K tokens, randomness but more likely to be reasonable.
		Top P ---> predictions combine probability doesnt exceed e.g 0.3
		
		Temperature: higher --> higher randomness
		
	======================================
	GenAI lifecycle:
		
		Scope ---> Select ----> Adapt and align model ----> Application integration
		
		
		Scope:
			Define as accurately and narrowingly.
				What functions:
					Many different tasks?
					single task like NER?
					
		Select:
			Choose model. Generally base model 
		
		Adapt and Align Model:
			Prompt Engineering --> in context learning.
			
			Fine tuning --> supervised learning process.
			
			Align with human feedback (reinforcement learning with human feedback)
			
			Evaluate
		
		Application Integration:
			optimize and deploy model for inference.
			Augment model and build LLM powered applications.
			
	-----------------------------------	
	Amazon SageMaker
	
		s3 bucket take resources for lab
		
		pytorch
		torchdata
		
		huggingface --> transformers	datasets.
		
		context window is important 
		anything above 4-5 few shot doesnt work generally
		
		
			
	================================
	LLM PRETRAINING AND SCALING LAWS

	
	Model hubs ---> strengths etc and info so cool
	
	pre-training at a high level. Vast amount of unstructured data.
	
	self-supervised learning step. structure etc of language learned.
	model weights trained to minimize loss of training objective.
	encoder --> trains token vector representation.
	
	1-3% tokens only used for pre training.
	
	Auto-Encoding
	encoder only trained using masked lnaguage modeling (MLM)
		aka auto-encoding
	
		sentiment analysis
		NER
		word classification
		
	AutoRegressive models (decoder only)	Casual Language modeling (CLM)
		Objective: predict next token. model iterates one by one to predict.
		context is unidirectional in autoregressive
		
		TExt Generation.
		Other emergent behavior
		
	
	Sequence to Sequence models:
		Span Corruption: <MASK> tokens <X> sentinel token added instead of mask.
		
		Reconstruct the span.
		
		Translation
		Text summarization
		Question answering.
		
		------> good for body of text as input and output.
		
	------------------------------------------
	Quantization ---> reduce memory required by reducing
						32 bit to 16bit floating point 
	
	BFLOAT16 --> brainfloating point format. hybrid b/w top 2
	not good for integer calculations
	
	4GB for 1B to 2GB in 16-b 8Bit -> 1GB
	
	---multi gpus 
		-Distribute Data Parrallel (DDP)
			copy model to each gpu, batches, each data simultanous and synchronized.
			
			model weights, gratients etc fit on one gpu.
		-Fully Sharded Data Parallel (FSDP)
			Sharding data 0 = data overlap, model doesnt fit in one gpu
		
		optimizer state == most GPU RAM
		
		Zero/sharding ---> distributes all in all gpu (gradients parameters ,optimizer states.
		gradients/optimizer and parameters can be sharded with diff stages 
		
		can work with models too big for one chip. FSDP(GET WEIGHTS B4 Forward and backward pass)
		get weights from all gpus. then synchronize at final step.
		
		sharding factor == communication volume increases with more sharding
		
	Combining data parallelism with pipeline parallelism is known as 2D parallelism.
	We can achieve 3D parallelism by combining data parallelism 
	with both pipeline parallelism and tensor parallelism simultaneously.
	----------------------------
	Goal: maximize model performance (minimize loss)
	
	Scaling choice dataset size (number of tokens)
	Gpu ram etc issue.
	
	compute budget --- dataset size --- model size 
	CAN DO PERFORMANCE TEST
	
	compute budget usually hard constraint. 
		hardware,time,budget
		
		*****chinchilla method == ideal usually x20 tokens of parameters
		
	----------------------------
	======pre training for domain adaptation
		e.g Legal writing == using very specific words like mens rea etc
		
	pre training will make better models for this.
	
	bloomberg gpt = finance and general data.
		51% financial data 
		
		
===================================================
	INSTRUCTION FINE TUNING
	
	instructions prompts
	
		limitations of in-context 
			not great in small models.
			takes context window space.
		
	Instruction fine tuning:
		classify this review:
		I don't like this review:
		Sentiment: negative
		
	prompt ---> completion
	multiple these.
	
	prompt will give instructions.
	
	Full Fine Tuning === updates all parameters.
	Required enough resources for it.
	
	First step:
		Training data.
		
	training validation and test splits.
	
	fine tuning:
		LLM completion compared with Label:
		
		************probability distribution ---> compare ---> standard cross
		entropyfunction to calculate loss. update using calculated loss in
		standard backpropogation in several epochs.
		
		
	Summarize the following text:
	[ Example Text ]
	[ Example Completion ]
	500-1000 examples can work for good fine tuning
	
	*** Catastrophic Forgetting:
		full fine tuning process --> modifies weights of original LLM.
		
		can forget other tasks while improving only one task.
		
	Catastrophic forgetting is a problem in both supervised and unsupervised learning tasks.
	In unsupervised learning, it can occur when the model is trained on a new dataset
	that is different from the one used during pre-training.
	
	One way to mitigate catastrophic forgetting is by using regularization techniques to limit
	the amount of change that can be made to the weights of the model during training. This can
	help to preserve the information learned during earlier training phases and prevent overfitting
	to the new data.
	
	--------------------------------
	multi-task instruction fine tuning:
		
		Improve performance of model over multiple epochs and updated weights.
		
	---> drawback ---> 50k-100000 examples 
		FLAN ---> e.g --> FLAN-T5 -->
		
	** Including diff ways to say the same thing helps a model generalize things.
	
	
		
	
	
	
			
	