Generative AI use cases, project lifecycle, and model pre-training

Fine-tuning and evaluating large language models

Reinforcement learning and LLM-powered applications


===============================================================
How an LLM works:

	- Transformer architecture why it took off etc.
	
	- Gen AI project life cycle.
		foundation model off the shelf, or make my own model etc.
		


	
----------------------------------------------------------
	Gen AI --> subset of ML
		---> learn patterns in datasets of contents generated by humans.
		---> foundation models, BERT,GPT,LLaMa,FLANT-5, PALM
		
		LLM take human written instructions --> prompt
		Context window ---> like a few 1000 windows.
		
		Completion ---> complete prompts
		
	---------
	
	Chat tasks ---> Next word prediction: Basic Chatbot
	
	translation/natural lang to code/file to summary etc/information retrieval (NER)/
	augmenting llms with integrate with apis.
	
	-----------
	
	RNN --> limited. needs more scaling and needs to see more
			than prev few words
			
	
	ATTENTION IS ALL YOU NEED
	
	
	--------------
	Transformers
	--------------
	learns context of all words. applies weights.
	
	- attention weights applied to each other word in sentences during llm training.
	
	- improved encoding language ability a lot
	
	- ****Self Attention
			strongly connected to or paying attention to something.
	
	- Transformers Architecture 
		----> Encoder and Decoder.
		
		big statistical calculators. must tokenize words into numbers.
		
	****tokenization methods --> token Ids, to represent part of word or whole words.
	
	Embedding space ----> Each token is represented as a vector:
	Vector ---> learn to encode meaning and context of each individual sequence.
	
	eg vector of just 3 --> relate words closer calc diff with angles etc.
	
	positional encoding: ---> preserve word order and position of word in sentence.
	
	****Token embeddings + positional encoding.
	-----------
	***GOES TO SELF ATTENTION LAYER ---> Encoder and Decoder
	
	Multi-headed Self Attention. ---> multiple heads learn indibivdually.
					each self attention head learns diff aspects 12-100 may be possible.
	-----------			
	*****GOES TO FEED FORWARD NETWORK 
		Vector of logics for probability of each token.
		final softmax layer ---> probability for each word in vocabulary.
		one token --> higher prob.
		
		feed-forward network applies a point-wise fully connected layer
		to each position separately and identically. 
	
	================================
	Translation using Transformers:
	Sequence to sequence task
		encoder deep representation of structure and meaning of sentence
		--> decoder	startup input trigger to predict next token.
		Output --> through feedforward layer to softmax output.
		New token and new token put forward.
		
		
	****Encoder:
		Encodes inputs (prompts) with contextual understanding
			produces one vector per input token.
			
	****Decoder:
		accepts input tokens and generates new tokens.
		
	BERT = ENCODER ONLY MODEL.
	
	BART and T5 === Encoder decoder models.
	
	GPT, LLaMa === DECODER ONLY MODELS.
		
		
	------------------------------------
	Context Window = total size allowed for prompt.
	
	**in-context learning
	
		zero shot inference.
		
		one shot inference.
		
		few-shot inference.
		
	***configuration parameters.
		Different than training parameters --> give u control over things etc.
		
		max new tokens === cap on how many times it can go back on the model.
	
		softmax of words and probability. has complete dictionary.
		
		greedy decoding --> pick highest probability --> good for short generation.
		more natural ---> random sampling --> (weighted with probability).
	do_sample = true
	***Sample top K and Top P
		Top K ---> only choose from top K tokens, randomness but more likely to be reasonable.
		Top P ---> predictions combine probability doesnt exceed e.g 0.3
		
		Temperature: higher --> higher randomness
		
	======================================
	GenAI lifecycle:
		
		Scope ---> Select ----> Adapt and align model ----> Application integration
		
		
		Scope:
			Define as accurately and narrowingly.
				What functions:
					Many different tasks?
					single task like NER?
					
		Select:
			Choose model. Generally base model 
		
		Adapt and Align Model:
			Prompt Engineering --> in context learning.
			
			Fine tuning --> supervised learning process.
			
			Align with human feedback (reinforcement learning with human feedback)
			
			Evaluate
		
		Application Integration:
			optimize and deploy model for inference.
			Augment model and build LLM powered applications.
			
	-----------------------------------	
	Amazon SageMaker
	
		s3 bucket take resources for lab
		
		pytorch
		torchdata
		
		huggingface --> transformers	datasets.
		
		context window is important 
		anything above 4-5 few shot doesnt work generally
		
		
			
	================================
	LLM PRETRAINING AND SCALING LAWS

	
	Model hubs ---> strengths etc and info so cool
	
	pre-training at a high level. Vast amount of unstructured data.
	
	self-supervised learning step. structure etc of language learned.
	model weights trained to minimize loss of training objective.
	encoder --> trains token vector representation.
	
	1-3% tokens only used for pre training.
	
	Auto-Encoding
	encoder only trained using masked lnaguage modeling (MLM)
		aka auto-encoding
	
		sentiment analysis
		NER
		word classification
		
	AutoRegressive models (decoder only)	Casual Language modeling (CLM)
		Objective: predict next token. model iterates one by one to predict.
		context is unidirectional in autoregressive
		
		TExt Generation.
		Other emergent behavior
		
	
	Sequence to Sequence models:
		Span Corruption: <MASK> tokens <X> sentinel token added instead of mask.
		
		Reconstruct the span.
		
		Translation
		Text summarization
		Question answering.
		
		------> good for body of text as input and output.
		
	------------------------------------------
	Quantization ---> reduce memory required by reducing
						32 bit to 16bit floating point 
	
	BFLOAT16 --> brainfloating point format. hybrid b/w top 2
	not good for integer calculations
	
	4GB for 1B to 2GB in 16-b 8Bit -> 1GB
	
	---multi gpus 
		-Distribute Data Parrallel (DDP)
			copy model to each gpu, batches, each data simultanous and synchronized.
			
			model weights, gratients etc fit on one gpu.
		-Fully Sharded Data Parallel (FSDP)
			Sharding data 0 = data overlap, model doesnt fit in one gpu
		
		optimizer state == most GPU RAM
		
		Zero/sharding ---> distributes all in all gpu (gradients parameters ,optimizer states.
		gradients/optimizer and parameters can be sharded with diff stages 
		
		can work with models too big for one chip. FSDP(GET WEIGHTS B4 Forward and backward pass)
		get weights from all gpus. then synchronize at final step.
		
		sharding factor == communication volume increases with more sharding
		
	Combining data parallelism with pipeline parallelism is known as 2D parallelism.
	We can achieve 3D parallelism by combining data parallelism 
	with both pipeline parallelism and tensor parallelism simultaneously.
	----------------------------
	Goal: maximize model performance (minimize loss)
	
	Scaling choice dataset size (number of tokens)
	Gpu ram etc issue.
	
	compute budget --- dataset size --- model size 
	CAN DO PERFORMANCE TEST
	
	compute budget usually hard constraint. 
		hardware,time,budget
		
		*****chinchilla method == ideal usually x20 tokens of parameters
		
	----------------------------
	======pre training for domain adaptation
		e.g Legal writing == using very specific words like mens rea etc
		
	pre training will make better models for this.
	
	bloomberg gpt = finance and general data.
		51% financial data 
		
		
===================================================
	INSTRUCTION FINE TUNING
	
	instructions prompts
	
		limitations of in-context 
			not great in small models.
			takes context window space.
		
	Instruction fine tuning:
		classify this review:
		I don't like this review:
		Sentiment: negative
		
	prompt ---> completion
	multiple these.
	
	prompt will give instructions.
	
	Full Fine Tuning === updates all parameters.
	Required enough resources for it.
	
	First step:
		Training data.
		
	training validation and test splits.
	
	fine tuning:
		LLM completion compared with Label:
		
		************probability distribution ---> compare ---> standard cross
		entropyfunction to calculate loss. update using calculated loss in
		standard backpropogation in several epochs.
		
		
	Summarize the following text:
	[ Example Text ]
	[ Example Completion ]
	500-1000 examples can work for good fine tuning
	
	*** Catastrophic Forgetting:
		full fine tuning process --> modifies weights of original LLM.
		
		can forget other tasks while improving only one task.
		
	Catastrophic forgetting is a problem in both supervised and unsupervised learning tasks.
	In unsupervised learning, it can occur when the model is trained on a new dataset
	that is different from the one used during pre-training.
	
	One way to mitigate catastrophic forgetting is by using regularization techniques to limit
	the amount of change that can be made to the weights of the model during training. This can
	help to preserve the information learned during earlier training phases and prevent overfitting
	to the new data.
	
	--------------------------------
	multi-task instruction fine tuning:
		
		Improve performance of model over multiple epochs and updated weights.
		
	---> drawback ---> 50k-100000 examples 
		FLAN ---> e.g --> FLAN-T5 -->
		
	** Including diff ways to say the same thing helps a model generalize things.
	
	
	----------------------------------
	Model Evaluation

	traditional ML
	
		Accuracy == correct pred/total prediction
		
		for known outputs
		
	LLM 
		ROUGE OR BLEU
		n-gram
		
		ROUGE: 
			***Text summarization
			compares a summary to one or more reference summaries.
			
			Recall = unigrams matches/unigrams in reference = 1 
			Precision = unigram matches/ unigrams in output = 0.8
			
			F1 = both = 2 x precision x recall/precision + recall

		bi grams = ROUGE-2 = lower scores in example.
		
		Longest common subsequence (LCS):
		LCS value = used
		
		ROUGE = can give bad a good example
			
		BLEU:
			***Used for text translation.
			compares with human tagged ones.
			
			good for the quality of ML text.
			
			avg(precision across range of n-gram sizes)
			
	====================
	BENCHMARKS
	
		GLUE SUPERGLUE
		
			GLUE --> sentinment analysis and question answers etc.
			
			SuperGlue improved limitations.
			
		result pages for models.
		
	(MMLU) Massive MultiTask Language Understanding.
	
	Big Bench 
		Physics software developments maths etc
		
	****HELM frameworks
		Holistic Evaluations: 7 metrics like accuracy fairness bias toxiicty etc.
		
		
	=====================
	PEFT
		memory for gradient temp memory forward activation trianable weights optimiziation etc.
		
	Only updates small subset of parameters.
	Other techniques adds new layers.
	Frozen Weights. 15-20% LLM weights updated.
	
	---> smaller footprints.
	
	PEFT Methods:
	
		Selective:
			subset of initial LLM params fine-tuned.
			Mixed performance.
			
		Reparameterizations:
			Reduce params by using low-rank representative.
			LoRA
			
		Additive:
			Add trainable layers:
				Adapters.
				Soft Prompts: mnaipulative inputs.
					Prompt Tuning.
					
	----------------------
	LoRA Method -> Low Rank
	ADDS MATRICES OF PARAMETERS IN SELF ATTENTION USUALLY
		freezes most of original LLM weights
		
		inject 2 rank decomposition matrices
		
		Train the wieghts of small matrices
		matrix multiplied low ranked to be same as main
		add to original weights.
		
		just apply to self-attention is fine. can do it to others but 
		only at self attention as most params in LLMs = best performance gain
	
		512x64 = 32,768 trainable params.
		
		LoRa with rank r=8
			A has dimensions 8x64 = 512
			B has 512 x 8 = 4,096
		
		86% less params and 4608 total
		
	Train different rank decomsitions matrices for different tasks.
	
	Update weights before inference.
	
	switch weights and use multiple matrices for diff tasks.
	
	
	--------------------------
	PEFT
	SOFT Prompts
	prompt tuning:
	ADDS TOKENS
		add additional trainable tokens supervised process determeines value.
		
		soft prompt vectors --> same length as token vectors
		20-100 examples? usually good
		
	soft prompts --> virtual tokens can take w.e value in the vector space.
	xyz etc 
	
	full fine tuning v.s prompt tuning
	Weight of model updated. v.s weights of model frozen and soft prompt trained.
	million to billions v.s 10k to 100k max
	
	can use different prompts for mutliple tasks.
	
	interpretability of soft prompts:
		Trained tokens dont correspond to any known token. Nearest tokens have similar meanings.
		so word like representation learned.
		
	-------------------------------
	Approach: Soft prompts learn additional embeddings, while LoRA modifies the transformer layers
	by adding low-rank matrices.
	
	Scope: Soft prompts only modify the input prompt representations, whereas LoRA focuses on model layers
	(e.g., attention layers) to introduce task-specific adaptations.
	
	Efficiency: Both are efficient methods, but LoRA tends to be used for more significant changes
	to the modelâ€™s internals, while soft prompts are a more lightweight approach for task conditioning.
	
	LAB 2
	
	PEFT
	
===================================================
RELF -- Reinforcement Learning from Human Feedback
RLHF

	use db for facts and genAi is for reasoning etc.
	
	HHH -- helpful honest harmless
	
	fine tuning with human feedback:
		produces better responses than instruned and pre tuned models.
		
		RLHF
		
		maximize helpfulness, relevence, minimize harms.
		avoid dangerous topics.
		
		--> reinforcement learning:
				agent learns to make decisions acc to goal in env
				
				learns from actions --> in env recieves rewards.
				increase chances of success.
				
				rollout --> as exp --> highest high term rewards.
				
				RL Policy = LLM == Generate aligned text objective : HHH
				
				**state = current context.
				
				env llm context. 

				reward = how close to human preference. reward is difficult.
				human evaluate it. acc to some alignement. LLM updated iteratively,.
				
				reward model --> classify outputs.
				train second by smaller training data.
				
				rollout == sequence of acitons and state.
				
				